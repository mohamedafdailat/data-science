{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "id": "6ec322f8-7188-4d34-9ced-dcabfd0ff75c"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Downloading from https://www.kaggle.com/api/v1/datasets/download/unsdsn/world-happiness?dataset_version_number=1&file_name=2016.csv...\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16.7k/16.7k [00:00<00:00, 8.14MB/s]\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Downloading from https://www.kaggle.com/api/v1/datasets/download/robikscube/textocr-text-extraction-from-images-dataset?dataset_version_number=2&file_name=annot.parquet...\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 36.0M/36.0M [00:02<00:00, 16.7MB/s]"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Extracting zip of annot.parquet...\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Downloading from https://www.kaggle.com/api/v1/datasets/download/wyattowalsh/basketball?dataset_version_number=231&file_name=nba.sqlite...\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 426M/426M [00:10<00:00, 42.3MB/s] "
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Extracting zip of nba.sqlite...\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\n"
                },
                {
                    "ename": "ValueError",
                    "evalue": "Original column name season not in the dataset. Current columns in the dataset: ['person_id', 'player_name']",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[4], line 27\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Load a Dataset by executing a SQL query against a SQLite DB, then rename a column\u001b[39;00m\n\u001b[1;32m     21\u001b[0m dataset \u001b[38;5;241m=\u001b[39m kagglehub\u001b[38;5;241m.\u001b[39mdataset_load(\n\u001b[1;32m     22\u001b[0m     KaggleDatasetAdapter\u001b[38;5;241m.\u001b[39mHUGGING_FACE,\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwyattowalsh/basketball\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnba.sqlite\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m     sql_query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT person_id, player_name FROM draft_history\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m )\n\u001b[0;32m---> 27\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mrename_column(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseason\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/datasets/fingerprint.py:442\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m out \u001b[38;5;241m=\u001b[39m func(dataset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/datasets/arrow_dataset.py:2213\u001b[0m, in \u001b[0;36mDataset.rename_column\u001b[0;34m(self, original_column_name, new_column_name, new_fingerprint)\u001b[0m\n\u001b[1;32m   2211\u001b[0m dataset \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   2212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_column_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names:\n\u001b[0;32m-> 2213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2214\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal column name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_column_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2215\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2216\u001b[0m     )\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_column_name \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names:\n\u001b[1;32m   2218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew column name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_column_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2220\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease choose a column name which is not already in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2222\u001b[0m     )\n",
                        "\u001b[0;31mValueError\u001b[0m: Original column name season not in the dataset. Current columns in the dataset: ['person_id', 'player_name']"
                    ]
                }
            ],
            "source": "import kagglehub\nfrom kagglehub import KaggleDatasetAdapter\n# Load a Dataset with a specific version of a CSV, then remove a column\ndataset = kagglehub.dataset_load(\n    KaggleDatasetAdapter.HUGGING_FACE,\n    \"unsdsn/world-happiness/versions/1\",\n    \"2016.csv\",\n)\ndataset = dataset.remove_columns('Region')\n\n# Load a Dataset with specific columns from a parquet file, then split into test/train splits\ndataset = kagglehub.dataset_load(\n    KaggleDatasetAdapter.HUGGING_FACE,\n    \"robikscube/textocr-text-extraction-from-images-dataset\",\n    \"annot.parquet\",\n    pandas_kwargs={\"columns\": [\"image_id\", \"bbox\", \"points\", \"area\"]}\n)\ndataset_with_splits = dataset.train_test_split(test_size=0.8, train_size=0.2)\n\n# Load a Dataset by executing a SQL query against a SQLite DB, then rename a column\ndataset = kagglehub.dataset_load(\n    KaggleDatasetAdapter.HUGGING_FACE,\n    \"wyattowalsh/basketball\",\n    \"nba.sqlite\",\n    sql_query=\"SELECT person_id, player_name FROM draft_history\",\n)\ndataset = dataset.rename_column('season', 'year')"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "id": "7f5cb719-a6ad-4f78-9ac2-d2c5f31b8cf9"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting kagglehub\n  Downloading kagglehub-0.3.10-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: packaging in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from kagglehub) (23.2)\nRequirement already satisfied: pyyaml in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from kagglehub) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from kagglehub) (2.32.2)\nRequirement already satisfied: tqdm in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from kagglehub) (4.66.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests->kagglehub) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests->kagglehub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests->kagglehub) (1.26.19)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests->kagglehub) (2025.1.31)\nDownloading kagglehub-0.3.10-py3-none-any.whl (63 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: kagglehub\nSuccessfully installed kagglehub-0.3.10\nNote: you may need to restart the kernel to use updated packages.\n"
                }
            ],
            "source": "pip install kagglehub"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "id": "25880e55-ba28-43b3-91a4-107987d8d69a"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already satisfied: kagglehub[hf-datasets] in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (0.3.10)\nRequirement already satisfied: packaging in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from kagglehub[hf-datasets]) (23.2)\nRequirement already satisfied: pyyaml in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from kagglehub[hf-datasets]) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from kagglehub[hf-datasets]) (2.32.2)\nRequirement already satisfied: tqdm in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from kagglehub[hf-datasets]) (4.66.4)\nCollecting datasets (from kagglehub[hf-datasets])\n  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: pandas in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from kagglehub[hf-datasets]) (2.1.4)\nRequirement already satisfied: filelock in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from datasets->kagglehub[hf-datasets]) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from datasets->kagglehub[hf-datasets]) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from datasets->kagglehub[hf-datasets]) (15.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from datasets->kagglehub[hf-datasets]) (0.3.7)\nCollecting xxhash (from datasets->kagglehub[hf-datasets])\n  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting multiprocess<0.70.17 (from datasets->kagglehub[hf-datasets])\n  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->kagglehub[hf-datasets]) (2023.10.0)\nRequirement already satisfied: aiohttp in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from datasets->kagglehub[hf-datasets]) (3.11.10)\nCollecting huggingface-hub>=0.24.0 (from datasets->kagglehub[hf-datasets])\n  Downloading huggingface_hub-0.29.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests->kagglehub[hf-datasets]) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests->kagglehub[hf-datasets]) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests->kagglehub[hf-datasets]) (1.26.19)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests->kagglehub[hf-datasets]) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pandas->kagglehub[hf-datasets]) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pandas->kagglehub[hf-datasets]) (2024.1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pandas->kagglehub[hf-datasets]) (2023.3)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from aiohttp->datasets->kagglehub[hf-datasets]) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from aiohttp->datasets->kagglehub[hf-datasets]) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from aiohttp->datasets->kagglehub[hf-datasets]) (23.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from aiohttp->datasets->kagglehub[hf-datasets]) (1.4.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from aiohttp->datasets->kagglehub[hf-datasets]) (6.0.4)\nRequirement already satisfied: propcache>=0.2.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from aiohttp->datasets->kagglehub[hf-datasets]) (0.2.0)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from aiohttp->datasets->kagglehub[hf-datasets]) (1.18.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets->kagglehub[hf-datasets]) (4.11.0)\nCollecting dill<0.3.9,>=0.3.0 (from datasets->kagglehub[hf-datasets])\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->kagglehub[hf-datasets]) (1.16.0)\nDownloading datasets-3.3.2-py3-none-any.whl (485 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.29.2-py3-none-any.whl (468 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m468.1/468.1 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.7\n    Uninstalling dill-0.3.7:\n      Successfully uninstalled dill-0.3.7\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.20.3\n    Uninstalling huggingface-hub-0.20.3:\n      Successfully uninstalled huggingface-hub-0.20.3\nSuccessfully installed datasets-3.3.2 dill-0.3.8 huggingface-hub-0.29.2 multiprocess-0.70.16 xxhash-3.5.0\nNote: you may need to restart the kernel to use updated packages.\n"
                }
            ],
            "source": "pip install kagglehub[hf-datasets]"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "id": "4b26db3a-59ec-4e91-8de9-2e6e88aef058"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "['person_id', 'player_name']\n"
                }
            ],
            "source": "print(dataset.column_names)"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "id": "b3ee508b-df74-4eee-8dc9-2312e3c27615"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Colonnes du dataset: ['person_id', 'player_name', 'year']\n"
                }
            ],
            "source": "import kagglehub\nfrom kagglehub import KaggleDatasetAdapter\n\n# Charger un Dataset \u00e0 partir d'un fichier CSV et supprimer une colonne\ndataset = kagglehub.dataset_load(\n    KaggleDatasetAdapter.HUGGING_FACE,\n    \"unsdsn/world-happiness/versions/1\",\n    \"2016.csv\",\n)\ndataset = dataset.remove_columns(\"Region\")\n\n# Charger un Dataset \u00e0 partir d'un fichier parquet et s\u00e9lectionner des colonnes sp\u00e9cifiques\ndataset = kagglehub.dataset_load(\n    KaggleDatasetAdapter.HUGGING_FACE,\n    \"robikscube/textocr-text-extraction-from-images-dataset\",\n    \"annot.parquet\",\n    pandas_kwargs={\"columns\": [\"image_id\", \"bbox\", \"points\", \"area\"]},\n)\ndataset_with_splits = dataset.train_test_split(test_size=0.8, train_size=0.2)\n\n# Charger un Dataset \u00e0 partir d'une requ\u00eate SQL et renommer une colonne (avec v\u00e9rification)\ndataset = kagglehub.dataset_load(\n    KaggleDatasetAdapter.HUGGING_FACE,\n    \"wyattowalsh/basketball\",\n    \"nba.sqlite\",\n    sql_query=\"SELECT person_id, player_name, season FROM draft_history\",\n)\n\n# V\u00e9rifier si la colonne 'season' existe avant de la renommer\nif \"season\" in dataset.column_names:\n    dataset = dataset.rename_column(\"season\", \"year\")\nelse:\n    print(\"\u26a0\ufe0f La colonne 'season' n'existe pas dans le dataset. V\u00e9rifiez la requ\u00eate SQL.\")\n\n# Afficher les colonnes pour confirmation\nprint(\"Colonnes du dataset:\", dataset.column_names)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "id": "90814c74-7987-4041-a403-49f48b8f54b8"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "['person_id', 'player_name', 'year']\n"
                }
            ],
            "source": "import kagglehub\nfrom kagglehub import KaggleDatasetAdapter\n\n# Load a Dataset with a specific version of a CSV, then remove a column\ndataset = kagglehub.dataset_load(\n    KaggleDatasetAdapter.HUGGING_FACE,\n    \"unsdsn/world-happiness/versions/1\",\n    \"2016.csv\",\n)\ndataset = dataset.remove_columns('Region')\n\n# Load a Dataset with specific columns from a parquet file, then split into test/train splits\ndataset = kagglehub.dataset_load(\n    KaggleDatasetAdapter.HUGGING_FACE,\n    \"robikscube/textocr-text-extraction-from-images-dataset\",\n    \"annot.parquet\",\n    pandas_kwargs={\"columns\": [\"image_id\", \"bbox\", \"points\", \"area\"]}\n)\ndataset_with_splits = dataset.train_test_split(test_size=0.8, train_size=0.2)\n\n# Load a Dataset by executing a SQL query against a SQLite DB, then rename a column\ndataset = kagglehub.dataset_load(\n    KaggleDatasetAdapter.HUGGING_FACE,\n    \"wyattowalsh/basketball\",\n    \"nba.sqlite\",\n    sql_query=\"SELECT person_id, player_name, season FROM draft_history\",\n)\n\n# Verify if 'season' column exists before renaming\nif 'season' in dataset.column_names:\n    dataset = dataset.rename_column('season', 'year')\nelse:\n    print(\"Column 'season' not found in the dataset\")\n\n# Verify the columns of the dataset\nprint(dataset.column_names)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "aa3d6abb-0b9b-4da0-8d40-3d9092e51e2c"
            },
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.11",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}